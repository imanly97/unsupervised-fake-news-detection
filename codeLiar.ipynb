{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columnNames = [\"id\",\"text\", \"topic\", \"author\", \"pos\", \"location\", \"party\", \"false\", \"barely\", \"half\", \"mostly\", \"pantsonfire\", \"context\"]\n",
    "df = pd.read_csv(\"liar.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6750"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['party'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds the word count of article text and appends to the dataframe\n",
    "\n",
    "count_list = []\n",
    "for i in df['text']:\n",
    "    count = len(i.split())\n",
    "    count_list.append(count)\n",
    "    \n",
    "df.loc[:,'word_count'] = count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create binary ground truth for possible eval\n",
    "\n",
    "truth = []\n",
    "for i in df['label']:\n",
    "    \n",
    "    if (i == \"FALSE\" or i == \"pants-fire\"):\n",
    "        truth.append(0)\n",
    "    else:\n",
    "        truth.append(1)\n",
    "df.loc[:,'truthval'] = truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>america</th>\n",
       "      <th>american</th>\n",
       "      <th>americans</th>\n",
       "      <th>barack</th>\n",
       "      <th>billion</th>\n",
       "      <th>budget</th>\n",
       "      <th>care</th>\n",
       "      <th>clinton</th>\n",
       "      <th>country</th>\n",
       "      <th>cut</th>\n",
       "      <th>federal</th>\n",
       "      <th>government</th>\n",
       "      <th>health</th>\n",
       "      <th>hillary</th>\n",
       "      <th>jobs</th>\n",
       "      <th>law</th>\n",
       "      <th>million</th>\n",
       "      <th>money</th>\n",
       "      <th>obama</th>\n",
       "      <th>office</th>\n",
       "      <th>pay</th>\n",
       "      <th>plan</th>\n",
       "      <th>president</th>\n",
       "      <th>public</th>\n",
       "      <th>rate</th>\n",
       "      <th>republican</th>\n",
       "      <th>spending</th>\n",
       "      <th>tax</th>\n",
       "      <th>taxes</th>\n",
       "      <th>texas</th>\n",
       "      <th>time</th>\n",
       "      <th>united</th>\n",
       "      <th>voted</th>\n",
       "      <th>wisconsin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6745</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6746</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6747</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6748</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6749</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6750 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      america  american  americans  barack  billion  budget  care  clinton  \\\n",
       "0           0         0          0       0        0       0     0        0   \n",
       "1           0         0          0       0        0       0     0        0   \n",
       "2           0         0          0       0        0       0     0        1   \n",
       "3           0         0          0       0        0       0     0        0   \n",
       "4           0         0          0       0        0       0     0        0   \n",
       "...       ...       ...        ...     ...      ...     ...   ...      ...   \n",
       "6745        0         0          0       0        0       0     0        0   \n",
       "6746        0         0          0       0        0       0     0        0   \n",
       "6747        0         0          0       0        0       0     0        0   \n",
       "6748        0         0          0       0        0       0     0        0   \n",
       "6749        0         0          0       0        0       0     0        0   \n",
       "\n",
       "      country  cut  federal  government  health  hillary  jobs  law  million  \\\n",
       "0           0    0        0           0       0        0     0    0        0   \n",
       "1           0    0        0           0       0        0     0    0        0   \n",
       "2           0    0        0           0       0        1     0    0        0   \n",
       "3           0    0        0           0       0        0     0    0        0   \n",
       "4           0    0        0           0       0        0     0    0        0   \n",
       "...       ...  ...      ...         ...     ...      ...   ...  ...      ...   \n",
       "6745        0    0        0           0       0        0     0    0        0   \n",
       "6746        0    0        0           0       0        0     0    0        0   \n",
       "6747        0    0        0           0       0        0     0    0        0   \n",
       "6748        0    0        0           0       0        0     0    0        0   \n",
       "6749        0    0        0           0       0        0     0    0        0   \n",
       "\n",
       "      money  obama  office  pay  plan  president  public  rate  republican  \\\n",
       "0         0      0       0    0     0          0       0     0           0   \n",
       "1         0      0       0    0     0          1       0     0           0   \n",
       "2         0      0       0    0     0          0       0     0           0   \n",
       "3         0      0       0    0     0          0       0     0           0   \n",
       "4         0      0       0    0     0          0       0     0           0   \n",
       "...     ...    ...     ...  ...   ...        ...     ...   ...         ...   \n",
       "6745      0      0       0    0     0          0       0     0           0   \n",
       "6746      0      0       0    0     0          0       0     0           0   \n",
       "6747      0      0       0    0     0          0       1     0           0   \n",
       "6748      0      0       0    0     0          0       0     0           0   \n",
       "6749      0      0       0    0     0          0       0     0           0   \n",
       "\n",
       "      spending  tax  taxes  texas  time  united  voted  wisconsin  \n",
       "0            0    0      0      0     0       0      0          0  \n",
       "1            0    0      0      0     0       0      0          0  \n",
       "2            0    0      0      0     0       0      0          0  \n",
       "3            0    0      0      0     0       0      0          0  \n",
       "4            0    0      0      0     0       0      0          0  \n",
       "...        ...  ...    ...    ...   ...     ...    ...        ...  \n",
       "6745         0    0      0      0     0       0      0          0  \n",
       "6746         0    0      0      0     0       0      0          0  \n",
       "6747         0    0      0      0     0       0      0          0  \n",
       "6748         0    0      0      0     0       0      0          1  \n",
       "6749         0    0      0      0     0       0      0          0  \n",
       "\n",
       "[6750 rows x 34 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenize for LDA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "skl_stopwords = list(text.ENGLISH_STOP_WORDS)\n",
    "\n",
    "all_skl_stopwords = skl_stopwords + [\"00\", \"10\", \"000\", \"year\", \"years\", \"says\", \"just\", \"said\", 'state', 'states', 'percent', 'people', 'new']\n",
    "\n",
    "binary = CountVectorizer(binary=False, stop_words = all_skl_stopwords, min_df= 0.0199) \n",
    "binary_dm = binary.fit_transform(df['text'])\n",
    "\n",
    "\n",
    "pd.DataFrame(binary_dm.toarray(),columns = binary.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement LDA and retain topics\n",
    "\n",
    "def create_topics(model, feature_names, nob):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topics.append(topic_idx)\n",
    "        topics.append(([feature_names[i] for i in topic.argsort()[:-nob - 1:-1]]))\n",
    "        \n",
    "        \n",
    "    return topics\n",
    "from sklearn.decomposition import LatentDirichletAllocation as lda\n",
    "\n",
    "LDA = lda(n_components = 5, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(binary_dm)\n",
    "\n",
    "topics = create_topics(LDA, binary.get_feature_names(), 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jobs', 'million', 'americans']\n",
      "['texas', 'united', 'voted']\n",
      "['federal', 'government', 'law']\n",
      "['obama', 'health', 'president']\n",
      "['tax', 'billion', 'taxes']\n"
     ]
    }
   ],
   "source": [
    "# drop topic indices\n",
    "for x in topics:\n",
    "    if (type(x) == int):\n",
    "        topics.remove(x)\n",
    "        \n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = nlp(\"This is a good example!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displacy.serve(sample, style = 'dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DET', 'AUX', 'DET', 'ADJ', 'NOUN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "print ([token.pos_ for token in sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignSpeech(listoftext):\n",
    "    \n",
    "    nouns = []\n",
    "    adjectives = []\n",
    "    adverbs = []\n",
    "    proper_nouns = []\n",
    "    for text in listoftext:\n",
    "        spacydText = nlp(text)\n",
    "        nounCount = 0\n",
    "        adjCount = 0\n",
    "        advCount = 0\n",
    "        pnounCount = 0\n",
    "        for token in spacydText:\n",
    "            if token.pos_ == 'NOUN':\n",
    "                nounCount += 1\n",
    "            if token.pos_ == 'ADJ':\n",
    "                adjCount += 1\n",
    "            if token.pos_ == 'ADV':\n",
    "                advCount += 1\n",
    "            if token.pos_ == 'PROPN':\n",
    "                pnounCount += 1\n",
    "        nouns.append(nounCount)\n",
    "        adjectives.append(adjCount)\n",
    "        adverbs.append(advCount)\n",
    "        proper_nouns.append(pnounCount)\n",
    "    \n",
    "    return nouns, adjectives, adverbs, proper_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns, adjectives, adverbs, proper_nouns = assignSpeech(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,'adjs'] = adjectives\n",
    "df.loc[:,'advbs'] = adverbs\n",
    "df.loc[:,'nouns'] = nouns\n",
    "df.loc[:,'pnouns'] = proper_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def toPercent(nouns):\n",
    "    #nounsPercent = []\n",
    "    #adjectivesPercent = []\n",
    "    #adverbsPercent = []\n",
    "    #proper_nounsPercent = []\n",
    "df['nounsPercent'] = df['nouns']/df['word_count']\n",
    "df['adjsPercent'] = df['adjs']/df['word_count']\n",
    "df['advbsPercent'] = df['advbs']/df['word_count']\n",
    "df['pnounsPercent'] = df['pnouns']/df['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding for location\n",
    "from sklearn import preprocessing\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "df['location']= label_encoder.fit_transform(df['location']) \n",
    "\n",
    "df['topic'] = label_encoder.fit_transform(df['topic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_token_compound</th>\n",
       "      <th>sentiment_result_token</th>\n",
       "      <th>sentiment_stem_compound</th>\n",
       "      <th>sentiment_result_stem</th>\n",
       "      <th>label</th>\n",
       "      <th>topic</th>\n",
       "      <th>author</th>\n",
       "      <th>pos</th>\n",
       "      <th>location</th>\n",
       "      <th>party</th>\n",
       "      <th>false</th>\n",
       "      <th>barely</th>\n",
       "      <th>half</th>\n",
       "      <th>mostly</th>\n",
       "      <th>pantsonfire</th>\n",
       "      <th>context</th>\n",
       "      <th>word_count</th>\n",
       "      <th>truthval</th>\n",
       "      <th>textCluster</th>\n",
       "      <th>adjs</th>\n",
       "      <th>advbs</th>\n",
       "      <th>nouns</th>\n",
       "      <th>pnouns</th>\n",
       "      <th>nounsPercent</th>\n",
       "      <th>adjsPercent</th>\n",
       "      <th>advbsPercent</th>\n",
       "      <th>pnounsPercent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>16</td>\n",
       "      <td>dwayne-bohac</td>\n",
       "      <td>State representative</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a mailer</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>half-true</td>\n",
       "      <td>1792</td>\n",
       "      <td>scott-surovell</td>\n",
       "      <td>State delegate</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a floor speech.</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>2001</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>President</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Denver</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.368421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Chicago Bears have had more starting quart...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>1620</td>\n",
       "      <td>robin-vos</td>\n",
       "      <td>Wisconsin Assembly speaker</td>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>a an online opinion-piece</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm the only person on this stage who has work...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>half-true</td>\n",
       "      <td>1850</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>President</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>a Democratic debate in Philadelphia, Pa.</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Says the Annies List political group supports ...   \n",
       "1  When did the decline of coal start? It started...   \n",
       "2  Hillary Clinton agrees with John McCain \"by vo...   \n",
       "3  The Chicago Bears have had more starting quart...   \n",
       "4  I'm the only person on this stage who has work...   \n",
       "\n",
       "   sentiment_token_compound sentiment_result_token  sentiment_stem_compound  \\\n",
       "0                       0.0                neutral                      0.0   \n",
       "1                       0.0                neutral                      0.0   \n",
       "2                       0.0                neutral                      0.0   \n",
       "3                       0.0                neutral                      0.0   \n",
       "4                       0.0                neutral                      0.0   \n",
       "\n",
       "  sentiment_result_stem        label  topic          author  \\\n",
       "0               neutral        FALSE     16    dwayne-bohac   \n",
       "1               neutral    half-true   1792  scott-surovell   \n",
       "2               neutral  mostly-true   2001    barack-obama   \n",
       "3               neutral         TRUE   1620       robin-vos   \n",
       "4               neutral    half-true   1850    barack-obama   \n",
       "\n",
       "                          pos  location  party  false  barely   half  mostly  \\\n",
       "0        State representative        56      1    0.0     1.0    0.0     0.0   \n",
       "1              State delegate        63      1    0.0     0.0    1.0     1.0   \n",
       "2                   President        18      1   70.0    71.0  160.0   163.0   \n",
       "3  Wisconsin Assembly speaker        73      1    0.0     3.0    2.0     5.0   \n",
       "4                   President        18      1   70.0    71.0  160.0   163.0   \n",
       "\n",
       "   pantsonfire                                   context  word_count  \\\n",
       "0          0.0                                  a mailer          11   \n",
       "1          0.0                           a floor speech.          24   \n",
       "2          9.0                                    Denver          19   \n",
       "3          1.0                 a an online opinion-piece          27   \n",
       "4          9.0  a Democratic debate in Philadelphia, Pa.          27   \n",
       "\n",
       "   truthval  textCluster  adjs  advbs  nouns  pnouns  nounsPercent  \\\n",
       "0         0            3     2      0      4       2      0.363636   \n",
       "1         1            0     1      2      5       4      0.208333   \n",
       "2         1            0     0      0      2       7      0.105263   \n",
       "3         1            0     3      1      5       3      0.185185   \n",
       "4         1            0     3      2      5       3      0.185185   \n",
       "\n",
       "   adjsPercent  advbsPercent  pnounsPercent  \n",
       "0     0.181818      0.000000       0.181818  \n",
       "1     0.041667      0.083333       0.166667  \n",
       "2     0.000000      0.000000       0.368421  \n",
       "3     0.111111      0.037037       0.111111  \n",
       "4     0.111111      0.074074       0.111111  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "- How many classes should be evaluated? T/F vs pants on fire, false, barely true, etc.\n",
    "- How to incorporate tokenized vectors?\n",
    "- How to incorporate the topic models? Assign each ob to a cluster\n",
    "- All features must be numeric for at least KMeans\n",
    "\n",
    "Clustering methods to try:\n",
    "    K-means\n",
    "    Spectral clustering\n",
    "    Hierarchical clustering\n",
    "    DBSCAN\n",
    "    OPTICS\n",
    "    Birch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vec = TfidfVectorizer(stop_words=\"english\")\n",
    "vec.fit(df.text.values)\n",
    "features = vec.transform(df.text.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.cluster import KMeans\\n\\n\\nSum_of_squared_distances = []\\nK = range(1,15)\\nfor k in K:\\n    km = KMeans(n_clusters=k)\\n    km = km.fit(features)\\n    Sum_of_squared_distances.append(km.inertia_)\\n'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "Sum_of_squared_distances = []\n",
    "K = range(1,15)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(features)\n",
    "    Sum_of_squared_distances.append(km.inertia_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nplt.plot(K, Sum_of_squared_distances, 'bx-')\\nplt.xlabel('k')\\nplt.ylabel('Sum_of_squared_distances')\\nplt.title('Elbow Method For Optimal k')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering of text vectors to create textCluster\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "cls = KMeans(n_clusters=8)\n",
    "cls.fit(features)\n",
    "preds = cls.predict(features)\n",
    "#print(len(cls.labels_))\n",
    "\n",
    "df['textCluster'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['textCluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.decomposition import PCA\\n\\npca = PCA(n_components=2)\\nreduced_features = pca.fit_transform(features.toarray())\\n\\nreduced_cluster_centers = pca.transform(cls.cluster_centers_)\\n'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "reduced_features = pca.fit_transform(features.toarray())\n",
    "\n",
    "reduced_cluster_centers = pca.transform(cls.cluster_centers_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nplt.scatter(reduced_features[:,0], reduced_features[:,1], c=cls.predict(features))\\nplt.scatter(reduced_cluster_centers[:, 0], reduced_cluster_centers[:,1], marker='x', s=100, c='b')\\n\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "plt.scatter(reduced_features[:,0], reduced_features[:,1], c=cls.predict(features))\n",
    "plt.scatter(reduced_cluster_centers[:, 0], reduced_cluster_centers[:,1], marker='x', s=100, c='b')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>party</th>\n",
       "      <th>word_count</th>\n",
       "      <th>textCluster</th>\n",
       "      <th>truthval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.015640</td>\n",
       "      <td>0.039576</td>\n",
       "      <td>-0.049402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>party</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_count</th>\n",
       "      <td>0.015640</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.015040</td>\n",
       "      <td>0.070266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textCluster</th>\n",
       "      <td>0.039576</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.015040</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.018520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>truthval</th>\n",
       "      <td>-0.049402</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.070266</td>\n",
       "      <td>0.018520</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             location  party  word_count  textCluster  truthval\n",
       "location     1.000000    NaN    0.015640     0.039576 -0.049402\n",
       "party             NaN    NaN         NaN          NaN       NaN\n",
       "word_count   0.015640    NaN    1.000000    -0.015040  0.070266\n",
       "textCluster  0.039576    NaN   -0.015040     1.000000  0.018520\n",
       "truthval    -0.049402    NaN    0.070266     0.018520  1.000000"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#correlation matrix of numeric feature\n",
    "\n",
    "df[['location','party','word_count','textCluster','truthval']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.util import *\n",
    "from nltk import tokenize\n",
    "snowballstemmer = SnowballStemmer(\"english\")\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ndef text_cleaner(text):\\n    semiclean_text = []\\n    for body in text.split():\\n        body = re.sub(r\\'@[A-Za-z0-9_]+\\',\\'\\',body)\\n        body = re.sub(r\"http\\\\S+\", \"\", body)\\n        body = re.sub(r\"[0-9]*\", \"\", body)\\n        body = re.sub(r\"(”|“|-|\\\\+|`|#|,|;|\\\\|)*\", \"\", body)\\n        body = re.sub(r\"&amp\", \"\", body)\\n        body = body.lower()\\n        semiclean_text.append(body)\\n        #print(semiclean_text)\\n        \\n    return semiclean_text\\n\\ndef tokenization_and_stem(semiclean_text):\\n    total_token_ls = []\\n    total_snowballstemmer_token_ls = []\\n    #semiclean_text = \\'\\'.join(semiclean_text)\\n    #print(semiclean_text)\\n    for sentence in semiclean_text:\\n        #print(sentence)\\n        #sentence = \\'\\'.join(sentence)\\n        token_ls = []\\n        snowballstemmer_token_ls = []\\n        tokens = nltk.word_tokenize(sentence)\\n        #print(tokens)\\n        for token in tokens:\\n            #print(token)\\n            if token not in stopwords:\\n                token_ls.append(token)\\n                snowballstemmer_token_ls.append(snowballstemmer.stem(token))\\n        total_token_ls.append(token_ls)\\n        total_snowballstemmer_token_ls.append(snowballstemmer_token_ls)\\n        #print(total_token_ls)\\n   \\n        return total_token_ls, total_snowballstemmer_token_ls\\n\\n# Create lists with same length as dataframe column\\ndf_text = [0]*len(df[\\'text\\'])\\ntoken_ls = [0]*len(df[\\'text\\'])\\nsnowstemmer_token_ls = [0]*len(df[\\'text\\'])\\n\\nfor num, text in enumerate(df[\\'text\\']):\\n    result = tokenization_and_stem(text_cleaner(text))\\n    if result is not None:\\n        token_ls[num], snowstemmer_token_ls[num] = tokenization_and_stem(text_cleaner(text))\\n#print(token_ls)\\n\\ndef back_to_clean_sent(token_ls):\\n    \"\"\"\\n    In order to perform sentiment analysis,\\n    here we put the words back into sentences. \\n    \"\"\"\\n    clean_sent_ls = []\\n    for word_ls in token_ls:\\n        clean_sent = \"\"\\n        for word in word_ls:\\n            clean_sent += (word + \" \")\\n        clean_sent_ls.append(clean_sent)\\n    return clean_sent_ls\\n\\n\\nsentence_tokenized = [0]*len(df[\"text\"])\\nfor num, token in enumerate(token_ls):\\n    if isinstance(token, int) == False:\\n        sentence_tokenized[num] = back_to_clean_sent(token)\\n#print(sentence_tokenized)\\n    \\nsentence_snowstemmeed = [0]*len(df[\"text\"])\\nfor num, token in enumerate(snowstemmer_token_ls):\\n    if isinstance(token, int) == False:\\n        sentence_snowstemmeed[num] = back_to_clean_sent(token)\\n        \\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\\nanalyser = SentimentIntensityAnalyzer()\\n\\ndef sentiment_analysis(insert_processed_sentence):\\n    \"\"\"\\n    Do the sentiment analysis\\n    \"\"\"\\n    sentiment = [0]*len(df[\"text\"])\\n    for num, player_sent in enumerate(insert_processed_sentence):\\n        ll = []\\n        if isinstance(player_sent, int) == False:\\n            for sentence in player_sent:\\n                ss = analyser.polarity_scores(sentence)\\n                ll.append(ss)\\n            sentiment[num] = ll\\n    return sentiment\\n\\n\\n#sentiment_original = sentiment_analysis(df[\"text\"])\\nsentiment_token = sentiment_analysis(sentence_tokenized)\\nsentiment_snowstemmed = sentiment_analysis(sentence_snowstemmeed)\\n\\n# Compound Score: \\n    # calculated by summing the valence scores of each word in the lexicon\\n    # normalized to be between -1 (maximum of the negative score) and +1 (maximum of the positive score)\\n    # ≥ 0.05 is a positive\\n    # ≤ -0.05 is a negative\\n    # ≥ 0.05 and ≤ -0.05 is neutral\\n    \\nnew_df = pd.DataFrame()\\ni = 0\\nfor senti_token, snow_stem in zip(sentiment_token, sentiment_snowstemmed):\\n    senti_token_compound = []\\n    senti_stem_compound = []\\n    sentiment_result_token = []\\n    sentiment_result_stem = []\\n\\n    if isinstance(senti_token, int) == False:\\n        for s1 in senti_token:\\n            senti_token_compound.append(s1[\"compound\"])\\n            if s1[\"compound\"] >= 0.05:\\n                sentiment_result_token.append(\"positive\")\\n            elif s1[\"compound\"] <= -0.05:\\n                sentiment_result_token.append(\"negative\")\\n            else:\\n                sentiment_result_token.append(\"neutral\")\\n                \\n    if isinstance(snow_stem, int) == False:        \\n        for s2 in snow_stem:\\n            senti_stem_compound.append(s2[\"compound\"])\\n            if s2[\"compound\"] >= 0.05:\\n                sentiment_result_stem.append(\"positive\")\\n            elif s2[\"compound\"] <= -0.05:\\n                sentiment_result_stem.append(\"negative\")\\n            else:\\n                sentiment_result_stem.append(\"neutral\")\\n    \\n    \\n    tw_df = pd.DataFrame.from_dict({\"text\":df[\"text\"][i],\\n                                \"sentiment_token_compound\":senti_token_compound,\\n                                \"sentiment_result_token\" :sentiment_result_token, \\n                                \"sentiment_stem_compound\":senti_stem_compound,\\n                                \"sentiment_result_stem\":sentiment_result_stem})\\n    new_df = new_df.append(tw_df)\\n    i += 1\\n    \\n    \\ndf_merged = new_df.merge(df, how=\\'inner\\', on=\\'text\\')\\ndf = df_merged\\n'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_cleaner(text):\n",
    "    semiclean_text = []\n",
    "    for body in text.split():\n",
    "        body = re.sub(r'@[A-Za-z0-9_]+','',body)\n",
    "        body = re.sub(r\"http\\S+\", \"\", body)\n",
    "        body = re.sub(r\"[0-9]*\", \"\", body)\n",
    "        body = re.sub(r\"(”|“|-|\\+|`|#|,|;|\\|)*\", \"\", body)\n",
    "        body = re.sub(r\"&amp\", \"\", body)\n",
    "        body = body.lower()\n",
    "        semiclean_text.append(body)\n",
    "        #print(semiclean_text)\n",
    "        \n",
    "    return semiclean_text\n",
    "\n",
    "def tokenization_and_stem(semiclean_text):\n",
    "    total_token_ls = []\n",
    "    total_snowballstemmer_token_ls = []\n",
    "    #semiclean_text = ''.join(semiclean_text)\n",
    "    #print(semiclean_text)\n",
    "    for sentence in semiclean_text:\n",
    "        #print(sentence)\n",
    "        #sentence = ''.join(sentence)\n",
    "        token_ls = []\n",
    "        snowballstemmer_token_ls = []\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        #print(tokens)\n",
    "        for token in tokens:\n",
    "            #print(token)\n",
    "            if token not in stopwords:\n",
    "                token_ls.append(token)\n",
    "                snowballstemmer_token_ls.append(snowballstemmer.stem(token))\n",
    "        total_token_ls.append(token_ls)\n",
    "        total_snowballstemmer_token_ls.append(snowballstemmer_token_ls)\n",
    "        #print(total_token_ls)\n",
    "   \n",
    "        return total_token_ls, total_snowballstemmer_token_ls\n",
    "\n",
    "# Create lists with same length as dataframe column\n",
    "df_text = [0]*len(df['text'])\n",
    "token_ls = [0]*len(df['text'])\n",
    "snowstemmer_token_ls = [0]*len(df['text'])\n",
    "\n",
    "for num, text in enumerate(df['text']):\n",
    "    result = tokenization_and_stem(text_cleaner(text))\n",
    "    if result is not None:\n",
    "        token_ls[num], snowstemmer_token_ls[num] = tokenization_and_stem(text_cleaner(text))\n",
    "#print(token_ls)\n",
    "\n",
    "def back_to_clean_sent(token_ls):\n",
    "    \"\"\"\n",
    "    In order to perform sentiment analysis,\n",
    "    here we put the words back into sentences. \n",
    "    \"\"\"\n",
    "    clean_sent_ls = []\n",
    "    for word_ls in token_ls:\n",
    "        clean_sent = \"\"\n",
    "        for word in word_ls:\n",
    "            clean_sent += (word + \" \")\n",
    "        clean_sent_ls.append(clean_sent)\n",
    "    return clean_sent_ls\n",
    "\n",
    "\n",
    "sentence_tokenized = [0]*len(df[\"text\"])\n",
    "for num, token in enumerate(token_ls):\n",
    "    if isinstance(token, int) == False:\n",
    "        sentence_tokenized[num] = back_to_clean_sent(token)\n",
    "#print(sentence_tokenized)\n",
    "    \n",
    "sentence_snowstemmeed = [0]*len(df[\"text\"])\n",
    "for num, token in enumerate(snowstemmer_token_ls):\n",
    "    if isinstance(token, int) == False:\n",
    "        sentence_snowstemmeed[num] = back_to_clean_sent(token)\n",
    "        \n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_analysis(insert_processed_sentence):\n",
    "    \"\"\"\n",
    "    Do the sentiment analysis\n",
    "    \"\"\"\n",
    "    sentiment = [0]*len(df[\"text\"])\n",
    "    for num, player_sent in enumerate(insert_processed_sentence):\n",
    "        ll = []\n",
    "        if isinstance(player_sent, int) == False:\n",
    "            for sentence in player_sent:\n",
    "                ss = analyser.polarity_scores(sentence)\n",
    "                ll.append(ss)\n",
    "            sentiment[num] = ll\n",
    "    return sentiment\n",
    "\n",
    "\n",
    "#sentiment_original = sentiment_analysis(df[\"text\"])\n",
    "sentiment_token = sentiment_analysis(sentence_tokenized)\n",
    "sentiment_snowstemmed = sentiment_analysis(sentence_snowstemmeed)\n",
    "\n",
    "# Compound Score: \n",
    "    # calculated by summing the valence scores of each word in the lexicon\n",
    "    # normalized to be between -1 (maximum of the negative score) and +1 (maximum of the positive score)\n",
    "    # ≥ 0.05 is a positive\n",
    "    # ≤ -0.05 is a negative\n",
    "    # ≥ 0.05 and ≤ -0.05 is neutral\n",
    "    \n",
    "new_df = pd.DataFrame()\n",
    "i = 0\n",
    "for senti_token, snow_stem in zip(sentiment_token, sentiment_snowstemmed):\n",
    "    senti_token_compound = []\n",
    "    senti_stem_compound = []\n",
    "    sentiment_result_token = []\n",
    "    sentiment_result_stem = []\n",
    "\n",
    "    if isinstance(senti_token, int) == False:\n",
    "        for s1 in senti_token:\n",
    "            senti_token_compound.append(s1[\"compound\"])\n",
    "            if s1[\"compound\"] >= 0.05:\n",
    "                sentiment_result_token.append(\"positive\")\n",
    "            elif s1[\"compound\"] <= -0.05:\n",
    "                sentiment_result_token.append(\"negative\")\n",
    "            else:\n",
    "                sentiment_result_token.append(\"neutral\")\n",
    "                \n",
    "    if isinstance(snow_stem, int) == False:        \n",
    "        for s2 in snow_stem:\n",
    "            senti_stem_compound.append(s2[\"compound\"])\n",
    "            if s2[\"compound\"] >= 0.05:\n",
    "                sentiment_result_stem.append(\"positive\")\n",
    "            elif s2[\"compound\"] <= -0.05:\n",
    "                sentiment_result_stem.append(\"negative\")\n",
    "            else:\n",
    "                sentiment_result_stem.append(\"neutral\")\n",
    "    \n",
    "    \n",
    "    tw_df = pd.DataFrame.from_dict({\"text\":df[\"text\"][i],\n",
    "                                \"sentiment_token_compound\":senti_token_compound,\n",
    "                                \"sentiment_result_token\" :sentiment_result_token, \n",
    "                                \"sentiment_stem_compound\":senti_stem_compound,\n",
    "                                \"sentiment_result_stem\":sentiment_result_stem})\n",
    "    new_df = new_df.append(tw_df)\n",
    "    i += 1\n",
    "    \n",
    "    \n",
    "df_merged = new_df.merge(df, how='inner', on='text')\n",
    "df = df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('liarcodeData.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_token_compound</th>\n",
       "      <th>sentiment_result_token</th>\n",
       "      <th>sentiment_stem_compound</th>\n",
       "      <th>sentiment_result_stem</th>\n",
       "      <th>label</th>\n",
       "      <th>topic</th>\n",
       "      <th>author</th>\n",
       "      <th>pos</th>\n",
       "      <th>location</th>\n",
       "      <th>party</th>\n",
       "      <th>false</th>\n",
       "      <th>barely</th>\n",
       "      <th>half</th>\n",
       "      <th>mostly</th>\n",
       "      <th>pantsonfire</th>\n",
       "      <th>context</th>\n",
       "      <th>word_count</th>\n",
       "      <th>truthval</th>\n",
       "      <th>textCluster</th>\n",
       "      <th>adjs</th>\n",
       "      <th>advbs</th>\n",
       "      <th>nouns</th>\n",
       "      <th>pnouns</th>\n",
       "      <th>nounsPercent</th>\n",
       "      <th>adjsPercent</th>\n",
       "      <th>advbsPercent</th>\n",
       "      <th>pnounsPercent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>16</td>\n",
       "      <td>dwayne-bohac</td>\n",
       "      <td>State representative</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a mailer</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>half-true</td>\n",
       "      <td>1792</td>\n",
       "      <td>scott-surovell</td>\n",
       "      <td>State delegate</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a floor speech.</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>2001</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>President</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Denver</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.368421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Chicago Bears have had more starting quart...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>1620</td>\n",
       "      <td>robin-vos</td>\n",
       "      <td>Wisconsin Assembly speaker</td>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>a an online opinion-piece</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm the only person on this stage who has work...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>half-true</td>\n",
       "      <td>1850</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>President</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>a Democratic debate in Philadelphia, Pa.</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Says the Annies List political group supports ...   \n",
       "1  When did the decline of coal start? It started...   \n",
       "2  Hillary Clinton agrees with John McCain \"by vo...   \n",
       "3  The Chicago Bears have had more starting quart...   \n",
       "4  I'm the only person on this stage who has work...   \n",
       "\n",
       "   sentiment_token_compound sentiment_result_token  sentiment_stem_compound  \\\n",
       "0                       0.0                neutral                      0.0   \n",
       "1                       0.0                neutral                      0.0   \n",
       "2                       0.0                neutral                      0.0   \n",
       "3                       0.0                neutral                      0.0   \n",
       "4                       0.0                neutral                      0.0   \n",
       "\n",
       "  sentiment_result_stem        label  topic          author  \\\n",
       "0               neutral        FALSE     16    dwayne-bohac   \n",
       "1               neutral    half-true   1792  scott-surovell   \n",
       "2               neutral  mostly-true   2001    barack-obama   \n",
       "3               neutral         TRUE   1620       robin-vos   \n",
       "4               neutral    half-true   1850    barack-obama   \n",
       "\n",
       "                          pos  location  party  false  barely   half  mostly  \\\n",
       "0        State representative        56      1    0.0     1.0    0.0     0.0   \n",
       "1              State delegate        63      1    0.0     0.0    1.0     1.0   \n",
       "2                   President        18      1   70.0    71.0  160.0   163.0   \n",
       "3  Wisconsin Assembly speaker        73      1    0.0     3.0    2.0     5.0   \n",
       "4                   President        18      1   70.0    71.0  160.0   163.0   \n",
       "\n",
       "   pantsonfire                                   context  word_count  \\\n",
       "0          0.0                                  a mailer          11   \n",
       "1          0.0                           a floor speech.          24   \n",
       "2          9.0                                    Denver          19   \n",
       "3          1.0                 a an online opinion-piece          27   \n",
       "4          9.0  a Democratic debate in Philadelphia, Pa.          27   \n",
       "\n",
       "   truthval  textCluster  adjs  advbs  nouns  pnouns  nounsPercent  \\\n",
       "0         0            4     2      0      4       2      0.363636   \n",
       "1         1            6     1      2      5       4      0.208333   \n",
       "2         1            6     0      0      2       7      0.105263   \n",
       "3         1            5     3      1      5       3      0.185185   \n",
       "4         1            6     3      2      5       3      0.185185   \n",
       "\n",
       "   adjsPercent  advbsPercent  pnounsPercent  \n",
       "0     0.181818      0.000000       0.181818  \n",
       "1     0.041667      0.083333       0.166667  \n",
       "2     0.000000      0.000000       0.368421  \n",
       "3     0.111111      0.037037       0.111111  \n",
       "4     0.111111      0.074074       0.111111  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_domain = df[['party','location','word_count','textCluster', \"sentiment_stem_compound\", 'word_count','textCluster' ,'nounsPercent', 'advbsPercent', 'pnounsPercent', 'adjsPercent' ]]\n",
    "\n",
    "\n",
    "X = df[[ 'topic', 'truthval','party'  ]]\n",
    "\n",
    "model = KMeans(n_clusters = 2)\n",
    "model.fit(X)\n",
    "modelPreds = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIRCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs \n",
    "from sklearn.cluster import Birch \n",
    "\n",
    "#X, clusters = make_blobs(n_samples = 6724, centers = 2, cluster_std = 0.75, random_state = 0) \n",
    "  \n",
    "modelBirch = Birch(branching_factor = 40, n_clusters = 2, threshold = 0.75) \n",
    "modelBirch.fit(X)\n",
    "predsBirch = modelBirch.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predsBirch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "def create_cm(t1, t2, title):\n",
    "    cm = confusion_matrix(t1, t2)\n",
    "    print('precision score: ', precision_score(t1,t2))\n",
    "    print('recall score: ', recall_score(t1,t2))\n",
    "    plt.matshow(cm)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.34      0.29      1756\n",
      "           1       0.74      0.65      0.69      4994\n",
      "\n",
      "    accuracy                           0.57      6750\n",
      "   macro avg       0.50      0.50      0.49      6750\n",
      "weighted avg       0.61      0.57      0.59      6750\n",
      "\n",
      "precision score:  0.73806275579809\n",
      "recall score:  0.6499799759711654\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 604 1152]\n",
      " [1748 3246]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df['truthval'],modelPreds))\n",
    "create_cm(df['truthval'],modelPreds,\"KMeans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.38      0.30      1756\n",
      "           1       0.73      0.60      0.66      4994\n",
      "\n",
      "    accuracy                           0.54      6750\n",
      "   macro avg       0.49      0.49      0.48      6750\n",
      "weighted avg       0.61      0.54      0.56      6750\n",
      "\n",
      "precision score:  0.7317133038782523\n",
      "recall score:  0.5969162995594713\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 663 1093]\n",
      " [2013 2981]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df['truthval'],predsBirch))\n",
    "create_cm(df['truthval'],predsBirch,\"BIRCH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
